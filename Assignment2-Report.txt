For the first task, the Scene dataset covers pictures denoting outdoor scenery, separated into a group of training and test sets to predict what features are present. In this machine learning problem, the test accuracy is what’s being evaluated, comparing it between the polynomial kernel with parameter = 2 and the Gaussian kernel with parameter = 2. In order to solve the multi-label classification problem, 6 independent SVM classifiers were trained. For both test samples, the 6 outputs were combined to form the predicted label. The performance was then evaluated between the predicted and true labels by averaging over both test sets. The results of both models’ test accuracy are 61.85% for SVM with a polynomial kernel and 63.43% for SVM with a Gaussian kernel. The Gaussian kernel’s test accuracy is higher than the polynomial’s, which is most likely due to the Gaussian kernel’s ability to capture nonlinear relationships between the picture features and labels more effectively. The polynomial kernel’s accuracy is still good, which could potentially come from its ability to detect feature interactions since many of the features appear together in the pictures. Overall, both kernels performed well achieving good accuracies, but the Gaussian kernel would be preferred since it did outperform the polynomial kernel slightly. 
For the second task, the Seeds dataset contains different types of wheat based on 7 features. This is another machine learning problem, except with the use of k-means to determine the group placement of each wheat based on the features with k = 3, 5, 7. To implement K-means from scratch, Euclidean distance was used. The algorithm was run 10 times either stopping when the number of iterations exceeded 100 or when the change in SSE dropped below 0.001. The results printed the average SSE for each k as SSE = 587.9 for k = 3, 407.1 for k = 5, and 311.4 for k = 7. As can be observed, the average SSE decreases as the K values enlarge, which can be attributed to the fact that each centroid represents fewer data points, which accounts for a lower variance within the clusters. The difference in the numbers decreasing is smaller between k = 5 and k = 7, which indicates that the returns are diminishing and there is less of a benefit of clustering at k = 7 than when changing the clustering from k = 3 to k = 5.
For the third, and final, task, the German Credit Card dataset contains 1000 datapoints with 20 features, and the 21st feature recorded as a 1 or 2, is labeling each datapoint either as fraudulent or legitimate. As another machine learning activity, the training/test split will be 70/30 randomized. A Random Forest classifier with standardized features was used, and the F-scores are the chosen measures to properly evaluate each technique’s performance. I tested 4 techniques for comparison to determine which model is best suited for combating the class imbalance. The 4 techniques used were the (1) baseline model, the (2) random under-sampling model, the (3) Synthetic Minority Over-Sampling Technique (SMOTE), and the (4) Synthetic Minority Over-Sampling Technique with Edited Nearest Neighbors (SMOTE-ENN), each of the 4 models’ outputs containing both (a) F1-Macro and (b) F1-Weighted. The results for each model are (1a) 0.7196 and (1b) 0.7731, (2a) 0.6806 and (2b) 0.7121, (3a) 0.6933 and (3b) 0.7460, and (4a) 0.7136 and (4b) 0.7525. The baseline model with class weights performed the best at F1-Weighted = 0.7731 as the class weights handled the imbalance effectively, with the random under-sampling performing the worst at F1-Macro = 0.6806, most likely because under-sampling deletes majority-class samples. While SMOTE performed better than under-sampling because it added to the minority class, it did not perform better than the baseline model because it may have also added noise into the mix; this is why SMOTE-ENN did better than SMOTE and performed second-best overall because ENN deletes mislabeled and noisy samples. This falls in line with the fact that the weighted type considers the proportion for each label in the dataset given the presence of class imbalance within the German Credit Card dataset because the Random Forest model with a balanced class weight already performs well by handling imbalances through adjusting the class samples.
In conclusion, this assignment highlights how SVM kernel selection impacts the multi-label classification accuracy, how increasing the number of clusters affects SSE in k-means, and how effective the baseline Random Forest classifier with class weights addresses class imbalance.
 